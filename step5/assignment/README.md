## 미션: LLM Router 모델 **성능 및 인퍼런스 시간 개선** 과제

현재 우리 **LLM Router**는 “쿼리를 분석해 `gpt-o1`, `Claude`, `gemini-mini` 중 한 곳으로 라우팅”하는 **3클래스 분류 모델**입니다.  
하지만 **데이터 수가 적고 레이블 편중**이 심하며, **인퍼런스 속도** 역시 **RPS 100(초당 100건 처리) 기준으로 50ms 이하**를 달성해야 하는 **서비스 요구사항**이 있습니다.  
이 과제에서는 **성능(정확도, F1)과 인퍼런스 시간 모두 개선**을 위해 다양한 전략을 고민하고 실험하세요.

---

## 1. 과제 목표

1. **분류 정확도 개선**  
   - 불균형/소량 데이터 문제를 극복하고, 3클래스 예측 성능(Accuracy, F1) 상승
2. **인퍼런스 시간 최적화**  
   - **RPS 100** 기준(= 10ms~50ms 안팎)으로 모델 추론이 동작하도록  
   - 모델 구조/추론 파이프라인/배포 환경 등을 최적화해 **지연(latency)을 낮추기**

---

## 2. 구체적인 과제 내용

### (1) 데이터 편향 & 성능 개선

1. **데이터 분석**  
   - 레이블별 샘플 분포, 불균형도, 평균 문장 길이 등
2. **데이터 증강 / 추가 데이터 수집**  
   - 소수 클래스에 대한 Augmentation(역번역, paraphrase 등)  
   - 클래스별 샘플 균형을 맞추는 Re-weighting, Over-sampling 기법  
3. **하이퍼파라미터 튜닝 & 모델 구조 선택**  
   - ModernBERT(FlashAttention) vs DistilBERT vs 다른 사전학습 모델  
   - 러닝레이트, 배치 사이즈, Epoch 수, Optimizer, 정규화/Dropout 등  
4. **평가지표**  
   - Accuracy & F1, 클래스별 Precision/Recall, Confusion Matrix  
   - 개선 전/후 지표 비교

### (2) 인퍼런스 시간 최적화

1. **추론 속도 측정**  
   - 단일 스레드 vs 멀티 스레드, GPU vs CPU, 배치 추론 vs 실시간 추론  
   - **실제 RPS 100 환경**(동시 요청 100회/초) 시 지연 시간 측정
2. **최적화 아이디어**  
   - **FP16/bf16** 혼합 정밀도 추론  
   - TorchScript, ONNX, TensorRT 등 **모델 최적화** 툴 사용  
   - ModernBERT + FlashAttention로 **패딩 제거**, 메모리/속도 최적화  
   - **Distil 모델** 또는 **TinyBERT** 등 소형화 모델  
   - **배포 인프라**(FastAPI vs gRPC vs C++ 서빙) 고려
3. **목표**  
   - **RPS 100 시 50ms 이하**(p95 또는 p99 기준) 달성  
   - 모델 사이즈 vs 속도 vs 정확도 간 **트레이드오프** 분석

### (3) 결과 보고서 요구사항

1. **실험 환경 요약**  
   - 사용한 모델/데이터셋/하드웨어(GPU/CPU), 배치 크기, 병렬 처리 방식 등  
2. **성능(정확도) & 인퍼런스 시간 개선 전/후 비교**  
   - (a) 표나 그래프로 F1 상승, (b) 평균 혹은 p95/99 인퍼런스 레이턴시 감소  
3. **분석 & 한계점**  
   - 어떤 레이블에서 여전히 오분류가 나는지, 어떤 환경에서 속도가 만족스럽지 않은지  
   - 추가로 시도해볼 만한 기법(ONNX, pruning, quantization 등)  
4. **최종 결론**  
   - 어떤 **전략**(ex: DistilBERT + BF16 + ONNX)으로 **RPS 100 시 50ms 이하**를 만족하면서 **F1**을 어느 수준 이상으로 유지했는지

---

## 3. 제출물

1. **코드**  
   - **데이터 편향 개선**(증강, 샘플링)  
   - **모델 학습 & 하이퍼파라미터 튜닝**  
   - **인퍼런스 최적화**(혼합 정밀, 모델 변환 등)  
   - **속도 측정 스크립트**(예: timeit, locust, vegeta, ab 등으로 RPS 테스트)
2. **보고서**  
   - (a) 개선 전/후 지표(Accuracy, F1, 추론 지연시간) 비교표  
   - (b) 각 시도별 설정, 결과, 배운 점  
   - (c) 한계와 추가 시도 가능성
3. **제출 방법**
   - 본 Repository에 `step5/submission/깃허브_아이디` 폴더에 업로드한 후에 Pull Request 보내기

---

### 참고 팁

- **데이터 적을 때**:  
  - 초거대 모델보다는 작은 모델(혹은 ModernBERT) → 과적합 방지 & 속도 향상  
  - 증강/샘플링으로 **미니배치 다양성** 확보  
- **인퍼런스 최적화**:  
  - **Torch compile**(PyTorch 2.x), **ONNX** or **TensorRT** 변환, **quantization**  
  - 병렬 처리 vs 배치 처리 → 각각의 레이턴시 측정  
  - GPU vs CPU 성능 비교  
- **정량적 목표**:  
  - RPS 100 기준 50ms 이하(throughput & latency)  
  - **LLM Router**로서 **적절한 분류 정확도**(Accuracy/F1 > 일정 값) 유지  

이 과제를 통해 **모델 성능(정확도)**과 **실서비스 요구사항(추론 지연)** 사이의 트레이드오프를 직접 파악하고, **다양한 최적화 기법**을 접목해보시기 바랍니다.
